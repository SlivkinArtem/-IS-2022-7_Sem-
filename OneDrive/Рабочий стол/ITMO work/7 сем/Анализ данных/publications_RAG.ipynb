{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaV1JAhtTpfS"
      },
      "outputs": [],
      "source": [
        "!pip install pandas feedparser tqdm faiss-cpu symspellpy rapidfuzz openai==1.* scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "from rapidfuzz import process, fuzz\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "import feedparser\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "BdT7PVkKTscA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API = \"http://export.arxiv.org/api/query\"\n",
        "\n",
        "QUERY = \"machine learning\"\n",
        "csv_path = \"arxiv_min.csv\"\n",
        "\n",
        "def fetch(start: int, max_results: int):\n",
        "    params = {\n",
        "        \"search_query\": f\"all:{QUERY}\",\n",
        "        \"start\": start,\n",
        "        \"max_results\": max_results,\n",
        "    }\n",
        "    url = f\"{API}?{urllib.parse.urlencode(params)}\"\n",
        "    with urllib.request.urlopen(url, timeout=60) as r:\n",
        "        data = r.read()\n",
        "    return feedparser.parse(data)"
      ],
      "metadata": {
        "id": "_NoeRQDnTuZp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "start = 0\n",
        "while len(records) < 350:\n",
        "    feed = fetch(start, 100)\n",
        "    entries = feed.entries\n",
        "    if not entries:\n",
        "        break\n",
        "    for e in entries:\n",
        "        title = (e.title or \"\").strip()\n",
        "        summary = (getattr(e, \"summary\", \"\") or \"\").strip()\n",
        "        try:\n",
        "            authors = [a.name for a in e.authors]\n",
        "        except Exception:\n",
        "            authors = [getattr(e, \"author\", \"\")] if hasattr(e, \"author\") else []\n",
        "        pub_date = getattr(e, \"published\", \"\")\n",
        "        records.append({\n",
        "            \"title\": title,\n",
        "            \"authors\": \", \".join(authors),\n",
        "            \"summary\": summary,\n",
        "        })\n",
        "    start += len(entries)\n",
        "\n",
        "records = records[:350]\n",
        "\n",
        "with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"title\", \"authors\", \"summary\"])\n",
        "    w.writeheader()\n",
        "    w.writerows(records)"
      ],
      "metadata": {
        "id": "-WzEIXzTTyb0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "df = df.fillna({\"title\":\"\", \"authors\":\"\", \"summary\":\"\"})\n",
        "df = df[(df[\"summary\"].str.strip()!=\"\") & (df[\"title\"].str.strip()!=\"\")]\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "B7GFqZv-UKLL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"len_words\"] = df[\"summary\"].apply(lambda x: len(str(x).split()))\n",
        "print(df[\"len_words\"].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGc6JS9DGFrm",
        "outputId": "6f6fa5b3-a1dc-4eb4-d973-8481e088701b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    350.000000\n",
            "mean     157.202857\n",
            "std       53.237065\n",
            "min       30.000000\n",
            "25%      118.250000\n",
            "50%      156.000000\n",
            "75%      190.750000\n",
            "max      439.000000\n",
            "Name: len_words, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \"https://api.vsegpt.ru/v1/\"\n",
        "API_KEY = \"sk-or-vv-7952300267ec5efa67eab60c3d6504cfa712007953e185d3440cc09767a6e503\"\n",
        "EMB_MODEL = \"emb-openai/text-embedding-3-small\"\n",
        "CHAT_MODEL = \"gpt-4o-mini\"\n",
        "CSV_IN = \"arxiv_min.csv\"\n",
        "INDEX_PATH = \"faiss.index\"\n",
        "META_PATH = \"meta.json\"\n",
        "OUT_ANS = \"answers.csv\""
      ],
      "metadata": {
        "id": "AlhoqhdZhe0-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)"
      ],
      "metadata": {
        "id": "H7Kd-DNmmtny"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_chunks(text, chunk_words=150, overlap=15):\n",
        "    ws = re.split(r\"\\s+\", text.strip())\n",
        "    ws = [w for w in ws if w]\n",
        "    out, i = [], 0\n",
        "    n = len(ws)\n",
        "    while i < n:\n",
        "        j = min(i+chunk_words, n)\n",
        "        out.append(\" \".join(ws[i:j]))\n",
        "        if j == n: break\n",
        "        i = j - overlap if j - overlap > 0 else j\n",
        "    return out"
      ],
      "metadata": {
        "id": "mTkmBN3Zm2YD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_WORDS = 150\n",
        "OVERLAP = 15\n",
        "TOP_K = 6\n",
        "def build_chunks(df):\n",
        "    rows = []\n",
        "    for i, r in df.iterrows():\n",
        "        for j, t in enumerate(word_chunks(r[\"summary\"], CHUNK_WORDS, OVERLAP)):\n",
        "            rows.append({\"text\": t, \"doc_id\": i, \"chunk_id\": j, \"title\": r[\"title\"], \"authors\": r[\"authors\"]})\n",
        "    return rows"
      ],
      "metadata": {
        "id": "G_z-gLLpuihl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_from(df, meta, limit_words_per_chunk=60):\n",
        "    def tok(s):\n",
        "        s = s.lower()\n",
        "        s = re.sub(r\"[^a-z0-9а-яё\\-]+\", \" \", s)\n",
        "        return [w for w in s.split() if w]\n",
        "    bag = set()\n",
        "    for t in df[\"title\"].tolist() + df[\"authors\"].tolist():\n",
        "        bag.update(tok(t))\n",
        "    for m in meta:\n",
        "        bag.update(tok(\" \".join(m[\"text\"].split()[:limit_words_per_chunk])))\n",
        "    return sorted(bag)\n"
      ],
      "metadata": {
        "id": "rURIav5L0GhD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_texts(texts, batch=128):\n",
        "    vecs = []\n",
        "    for i in range(0, len(texts), batch):\n",
        "        batch_texts = texts[i:i+batch]\n",
        "        resp = client.embeddings.create(model=EMB_MODEL, input=batch_texts)\n",
        "        vecs.append(np.array([d.embedding for d in resp.data], dtype=np.float32))\n",
        "    X = np.vstack(vecs)\n",
        "    X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
        "    return X"
      ],
      "metadata": {
        "id": "bsT4tS0Tux_P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_faiss(X):\n",
        "    idx = faiss.IndexFlatIP(X.shape[1])\n",
        "    idx.add(X)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "m-_UVDGkue14"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_store(index, meta):\n",
        "    faiss.write_index(index, INDEX_PATH)\n",
        "    with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(meta, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "ZTNWQdgAupmk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_store():\n",
        "    idx = faiss.read_index(INDEX_PATH)\n",
        "    meta = json.load(open(META_PATH, \"r\", encoding=\"utf-8\"))\n",
        "    return idx, meta"
      ],
      "metadata": {
        "id": "pLyOOeQLu-Ro"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_query(q, vocab):\n",
        "    toks = re.sub(r\"[^a-z0-9а-яё\\-]+\", \" \", q.lower()).split()\n",
        "    v = vocab\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if t in v:\n",
        "            out.append(t)\n",
        "        else:\n",
        "            hit = process.extractOne(t, v, scorer=fuzz.QRatio, score_cutoff=85)\n",
        "            out.append(hit[0] if hit else t)\n",
        "    return \" \".join(out) if out else q"
      ],
      "metadata": {
        "id": "TPEzn6psxlt-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(index, meta, query, k=TOP_K):\n",
        "    qv = embed_texts([query])\n",
        "    D, I = index.search(qv, k)\n",
        "    res = []\n",
        "    for rank, (idx, sc) in enumerate(zip(I[0].tolist(), D[0].tolist()), 1):\n",
        "        if idx < 0: continue\n",
        "        item = dict(meta[idx])\n",
        "        item[\"rank\"] = rank\n",
        "        item[\"score\"] = float(sc)\n",
        "        res.append(item)\n",
        "    return res"
      ],
      "metadata": {
        "id": "gh-UmRROxoSw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(prompt):\n",
        "    r = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=800\n",
        "    )\n",
        "    return r.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "ecFhAdFnxtEG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_rag(q, hits):\n",
        "    ctx = []\n",
        "    for h in hits:\n",
        "        piece = \" \".join(h[\"text\"].split()[:160])\n",
        "        ctx.append(f\"- {h['title']} — {h['authors']}\\n{piece}\")\n",
        "    ctx = \"\\n\\n\".join(ctx)\n",
        "    prompt = f\"\"\"\n",
        "    You are a scientific assistant. Use THIS CONTEXT (fragments of arXiv abstracts) to answer.\n",
        "    If something is not found in the context, say \"not found in the provided corpus\".\n",
        "    Answer briefly, clearly, and include references to the article titles from the context.\n",
        "\n",
        "    User question: {q}\n",
        "\n",
        "    CONTEXT:\n",
        "    {ctx}\n",
        "\n",
        "    Instructions:\n",
        "    - Provide key ideas and results, mention methods/datasets/metrics if available.\n",
        "    - At the end, list up to 3 article titles and authors from the context that you based the answer on.\n",
        "    \"\"\"\n",
        "    return chat(prompt)"
      ],
      "metadata": {
        "id": "XtPF1YjlyHzh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not (os.path.exists(INDEX_PATH) and os.path.exists(META_PATH)):\n",
        "        meta = build_chunks(df)\n",
        "        X = embed_texts([m[\"text\"] for m in meta])\n",
        "        index = build_faiss(X)\n",
        "        save_store(index, meta)\n",
        "index, meta = load_store()\n",
        "vocab = vocab_from(df, meta)\n",
        "\n",
        "queries = [\n",
        "    \"Federeted learnnng with differential privacy: key ideas and challenges\",\n",
        "    \"Key concepts and architectures of transformer models in NLP\",\n",
        "    \"Methods for improving robustness and calibration of deep neural networks\",\n",
        "    \"Techniques for fine-tuning large language models\",\n",
        "    \"Causal representation learning: main goals, methods, and common pitfalls\"\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "        q2 = correct_query(q, vocab)\n",
        "        hits = search(index, meta, q2, TOP_K)\n",
        "        a_rag = answer_rag(q, hits)\n",
        "\n",
        "        print(\"Вопрос:\", q)\n",
        "        if q2 != q:\n",
        "            print(\"Исправленный запрос:\", q2)\n",
        "        print(\"\\n RAG: \\n\", a_rag)\n",
        "        print(\"\\nТоп статьи:\")\n",
        "        for h in hits:\n",
        "            print(f\"  [{h['rank']}] {h['title']} ({h['score']:.3f})\")"
      ],
      "metadata": {
        "id": "eZzAAVb-yLIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fcf6475-f498-4674-d74d-6a8a2b70894c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: Federeted learnnng with differential privacy: key ideas and challenges\n",
            "Исправленный запрос: federated learning with differential privacy key ideas and challenges\n",
            "\n",
            " RAG: \n",
            " Federated learning combined with differential privacy presents unique challenges and opportunities. Federated learning allows decentralized training of models without sharing sensitive data, which is crucial in sensitive domains like healthcare. However, ensuring patient privacy while maintaining model accuracy is complex. \n",
            "\n",
            "One study, \"Federated and Differentially Private Learning for Electronic Health Records\" by Stephen R. Pfohl et al., explores the efficacy of centralized versus federated learning in private and non-private settings, particularly for clinical prediction tasks. It highlights that while applying differentially private stochastic gradient descent is straightforward in centralized settings, it becomes significantly more challenging in federated contexts.\n",
            "\n",
            "Another relevant work, \"Federated Learning with Superquantile Aggregation for Heterogeneous Data\" by Krishna Pillutla et al., proposes a federated learning framework that uses a superquantile-based learning objective to handle heterogeneous data across clients. This approach incorporates differentially private client filtering with federated averaging, demonstrating competitive performance in terms of average error and improved tail statistics.\n",
            "\n",
            "Overall, the integration of differential privacy in federated learning requires careful consideration of privacy guarantees and model accuracy, particularly in decentralized environments.\n",
            "\n",
            "References:\n",
            "1. \"Federated and Differentially Private Learning for Electronic Health Records\" — Stephen R. Pfohl et al.\n",
            "2. \"Federated Learning with Superquantile Aggregation for Heterogeneous Data\" — Krishna Pillutla et al.\n",
            "3. \"Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle\" — Yu-Xiang Wang et al.\n",
            "\n",
            "Топ статьи:\n",
            "  [1] Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle (0.644)\n",
            "  [2] Federated and Differentially Private Learning for Electronic Health Records (0.621)\n",
            "  [3] Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle (0.608)\n",
            "  [4] Bayesian Differential Privacy for Machine Learning (0.606)\n",
            "  [5] Federated Learning with Superquantile Aggregation for Heterogeneous Data (0.599)\n",
            "  [6] Matrix Sketching for Secure Collaborative Machine Learning (0.564)\n",
            "Вопрос: Key concepts and architectures of transformer models in NLP\n",
            "Исправленный запрос: key concepts and architectures of transformer models in nlp\n",
            "\n",
            " RAG: \n",
            " Key concepts and architectures of transformer models in NLP include their ability to handle long-distance dependencies and their application in various tasks. The transformer architecture is highlighted for its effectiveness in learning from complex data structures, such as molecular representations in high throughput screening (Gurbych et al.) and in the context of machine learning for regular languages (van der Poel et al.). Additionally, transformers are utilized in hybrid models that integrate asynchronous and synchronous processing for real-time data (Martin-Turrero et al.), showcasing their versatility in different domains.\n",
            "\n",
            "1. \"High throughput screening with machine learning\" — Oleksandr Gurbych et al.\n",
            "2. \"MLRegTest: A Benchmark for the Machine Learning of Regular Languages\" — Sam van der Poel et al.\n",
            "3. \"ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data\" — Carmen Martin-Turrero et al.\n",
            "\n",
            "Топ статьи:\n",
            "  [1] High throughput screening with machine learning (0.484)\n",
            "  [2] MLRegTest: A Benchmark for the Machine Learning of Regular Languages (0.455)\n",
            "  [3] ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data (0.449)\n",
            "  [4] Open Vocabulary Learning on Source Code with a Graph-Structured Cache (0.442)\n",
            "  [5] Deep Learning and Its Applications to Machine Health Monitoring: A Survey (0.441)\n",
            "  [6] Physics-Inspired Interpretability Of Machine Learning Models (0.436)\n",
            "Вопрос: Methods for improving robustness and calibration of deep neural networks\n",
            "Исправленный запрос: methods for improving robustness and calibration of deep neural networks\n",
            "\n",
            " RAG: \n",
            " Methods for improving robustness and calibration of deep neural networks include:\n",
            "\n",
            "1. **Data-Centric Approaches**: The paper \"Towards Efficient Data-Centric Robust Machine Learning with Noise-based Augmentation\" discusses a noise-based data augmentation method that incorporates Gaussian Noise, Salt-and-Pepper noise, and PGD adversarial perturbations to enhance model robustness against unforeseen malicious inputs. This approach emphasizes the importance of building appropriate datasets to improve AI model performance.\n",
            "\n",
            "2. **Knowledge Integration**: The \"Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks\" proposes integrating domain knowledge into a probabilistic graphical model to enhance the robustness of deep neural networks against adversarial attacks. This method leverages logical relationships among predictions to improve the model's resilience.\n",
            "\n",
            "3. **Robustness to Dataset Drift**: The study \"Data Models for Dataset Drift Controls in Machine Learning With Optical Images\" addresses robustness concerns related to dataset drift by pairing traditional machine learning with physical optics to create explicit data models. This approach aims to improve understanding and performance when there are discrepancies between training and deployment data.\n",
            "\n",
            "These methods highlight the importance of data quality, domain knowledge, and understanding data generation processes in enhancing the robustness and calibration of deep neural networks.\n",
            "\n",
            "References:\n",
            "- \"Towards Efficient Data-Centric Robust Machine Learning with Noise-based Augmentation\" — Xiaogeng Liu et al.\n",
            "- \"Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks\" — Nezihe Merve Gürel et al.\n",
            "- \"Data Models for Dataset Drift Controls in Machine Learning With Optical Images\" — Luis Oala et al.\n",
            "\n",
            "Топ статьи:\n",
            "  [1] Detecting Moving Objects With Machine Learning (0.512)\n",
            "  [2] Towards Efficient Data-Centric Robust Machine Learning with Noise-based Augmentation (0.508)\n",
            "  [3] Data Models for Dataset Drift Controls in Machine Learning With Optical Images (0.505)\n",
            "  [4] Adaptive Second Order Coresets for Data-efficient Machine Learning (0.498)\n",
            "  [5] Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks (0.492)\n",
            "  [6] A Framework and Benchmark for Deep Batch Active Learning for Regression (0.492)\n",
            "Вопрос: Techniques for fine-tuning large language models\n",
            "Исправленный запрос: techniques for fine-tuning large language models\n",
            "\n",
            " RAG: \n",
            " The context does not provide specific techniques for fine-tuning large language models (LLMs). However, it mentions the use of LLMs in various frameworks and applications, such as:\n",
            "\n",
            "1. **Verbalized Machine Learning (VML)**: This framework allows for automatic model class selection and interpretable learner updates by using LLMs to encode prior knowledge in natural language, which can guide the optimization process (Xiao et al.).\n",
            "\n",
            "2. **Alpha MAML**: This method enhances model-agnostic meta-learning by incorporating an online hyperparameter adaptation scheme, which can improve training stability and reduce the need for hyperparameter tuning, relevant for fine-tuning tasks (Behl et al.).\n",
            "\n",
            "3. **LLM-based Feature Generation**: This approach explores using LLMs to extract interpretable features from text for machine learning tasks, demonstrating that features generated by LLMs can be semantically meaningful and effective for classification tasks (Balek et al.).\n",
            "\n",
            "While these articles discuss aspects related to LLMs and their applications, they do not specifically detail techniques for fine-tuning LLMs.\n",
            "\n",
            "**References**:\n",
            "1. Verbalized Machine Learning: Revisiting Machine Learning with Language Models — Tim Z. Xiao et al.\n",
            "2. Alpha MAML: Adaptive Model-Agnostic Meta-Learning — Harkirat Singh Behl et al.\n",
            "3. LLM-based feature generation from text for interpretable machine learning — Vojtěch Balek et al.\n",
            "\n",
            "Топ статьи:\n",
            "  [1] ML$^2$Tuner: Efficient Code Tuning via Multi-Level Machine Learning Models (0.512)\n",
            "  [2] Verbalized Machine Learning: Revisiting Machine Learning with Language Models (0.498)\n",
            "  [3] Verbalized Machine Learning: Revisiting Machine Learning with Language Models (0.493)\n",
            "  [4] Alpha MAML: Adaptive Model-Agnostic Meta-Learning (0.487)\n",
            "  [5] Open Vocabulary Learning on Source Code with a Graph-Structured Cache (0.480)\n",
            "  [6] LLM-based feature generation from text for interpretable machine learning (0.468)\n",
            "Вопрос: Causal representation learning: main goals, methods, and common pitfalls\n",
            "Исправленный запрос: causal representation learning main goals methods and common pitfalls\n",
            "\n",
            " RAG: \n",
            " Causal representation learning (CRL) aims to identify and utilize causal relationships between variables to improve generalization and robustness in machine learning models. The main goals of CRL include achieving identifiability of latent causal variables and ensuring that models can generalize well to unseen data distributions. This is particularly important in contexts where traditional models may fail due to their reliance on associative rather than causal relationships.\n",
            "\n",
            "Key methods in CRL involve intervention-based approaches, where the focus is on designing algorithms that can recover true latent causal structures. For instance, the paper \"Score-based Causal Representation Learning: Linear and General Transformations\" discusses how score functions can be leveraged to ensure both identifiability and achievability in causal models, emphasizing the importance of interventions for achieving these goals.\n",
            "\n",
            "Common pitfalls in CRL include the challenges of generalization in anti-causal learning settings, where models may struggle to infer causes from effects without a proper causal framework. The paper \"Generalization in anti-causal learning\" highlights the need for incorporating causal models into supervised learning to enhance the search and validation processes, thereby improving model performance in novel situations.\n",
            "\n",
            "Additionally, the paper \"Alleviating Privacy Attacks via Causal Learning\" illustrates how causal models can provide stronger guarantees against privacy attacks, further emphasizing the practical benefits of adopting causal approaches in machine learning.\n",
            "\n",
            "In summary, CRL focuses on understanding and leveraging causal relationships to improve model performance, with methods that emphasize intervention and identifiability, while also addressing common pitfalls related to generalization and privacy.\n",
            "\n",
            "References:\n",
            "1. \"Score-based Causal Representation Learning: Linear and General Transformations\" — Burak Varıcı et al.\n",
            "2. \"Generalization in anti-causal learning\" — Niki Kilbertus et al.\n",
            "3. \"Alleviating Privacy Attacks via Causal Learning\" — Shruti Tople et al.\n",
            "\n",
            "Топ статьи:\n",
            "  [1] Generalization in anti-causal learning (0.571)\n",
            "  [2] Score-based Causal Representation Learning: Linear and General Transformations (0.551)\n",
            "  [3] Generalization in anti-causal learning (0.531)\n",
            "  [4] Alleviating Privacy Attacks via Causal Learning (0.499)\n",
            "  [5] Mislabeled examples detection viewed as probing machine learning models: concepts, survey and extensive benchmark (0.452)\n",
            "  [6] A Scoping Review of Earth Observation and Machine Learning for Causal Inference: Implications for the Geography of Poverty (0.448)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McnNDnZeH_PH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}