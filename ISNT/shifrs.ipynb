{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8neJC0KcHUtl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def read_sheet(ref):\n",
        "    m = re.search(r'(\\d+)\\s*(об\\.)?', ref)\n",
        "    if not m:\n",
        "        return None\n",
        "    return int(m.group(1)), bool(m.group(2))\n",
        "\n",
        "\n",
        "def sheet_to_value(n, ob):\n",
        "    return 2 * n + (1 if ob else 0)\n",
        "\n",
        "\n",
        "def value_to_sheet(v):\n",
        "    return v // 2, (v % 2 == 1)\n",
        "\n",
        "\n",
        "def expand_range_piece(segment):\n",
        "    segment = segment.strip()\n",
        "    if re.search(r'[–—-]', segment):\n",
        "        left, right = re.split(r'[–—-]', segment, maxsplit=1)\n",
        "        a = read_sheet(left.strip())\n",
        "        b = read_sheet(right.strip())\n",
        "        if not a or not b:\n",
        "            return []\n",
        "        av = sheet_to_value(*a)\n",
        "        bv = sheet_to_value(*b)\n",
        "        step = 1 if bv >= av else -1\n",
        "\n",
        "        out = []\n",
        "        for v in range(av, bv + step, step):\n",
        "            n, ob = value_to_sheet(v)\n",
        "            out.append(f\"{n} об.\" if ob else f\"{n}\")\n",
        "        return out\n",
        "\n",
        "    p = read_sheet(segment)\n",
        "    if not p:\n",
        "        return []\n",
        "    n, ob = p\n",
        "    return [f\"{n} об.\" if ob else f\"{n}\"]\n",
        "\n",
        "\n",
        "def extract_sheets(block):\n",
        "    # убираем цитаты и круглые скобки (описания, даты и т.п.)\n",
        "    block = re.sub(r'«[^»]*»', '', block)\n",
        "    block = re.sub(r'\\([^()]*\\)', '', block)\n",
        "    # выкидываем упоминания других ПД внутри блока листов\n",
        "    block = re.sub(r'ПД\\s*\\d+', '', block)\n",
        "    # убираем \"л.\"\n",
        "    block = re.sub(r'л\\.\\s*', '', block)\n",
        "\n",
        "    pattern = r'\\d+\\s*(?:об\\.)?(?:\\s*[–—-]\\s*\\d+\\s*(?:об\\.)?)?'\n",
        "    segments = re.findall(pattern, block)\n",
        "\n",
        "    out = []\n",
        "    for seg in segments:\n",
        "        out.extend(expand_range_piece(seg))\n",
        "    return out\n",
        "\n",
        "\n",
        "def generate_full_pd(pd_name, limit=5):\n",
        "    out = []\n",
        "    for i in range(1, limit + 1):\n",
        "        out.append(f\"{pd_name} л. {i}\")\n",
        "        out.append(f\"{pd_name} л. {i} об.\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def expand_pd_interval(start, end, limit=5):\n",
        "    out = []\n",
        "    step = 1 if end >= start else -1\n",
        "    for num in range(start, end + step, step):\n",
        "        out.extend(generate_full_pd(f\"ПД {num}\", limit))\n",
        "    return out\n",
        "\n",
        "\n",
        "def parse_record(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    s = str(text)\n",
        "\n",
        "    # \"в тетр. 834\" -> \"в тетр. ПД 834\"\n",
        "    s = re.sub(r'(тетр\\.\\s*)(\\d+)', r'\\1ПД \\2', s)\n",
        "    # \"ПД, ф. 244\" -> \"ПД 244\"\n",
        "    s = re.sub(r'ПД,\\s*ф\\.\\s*(\\d+)', r'ПД \\1', s)\n",
        "\n",
        "    # блок листов: \"л. ...\" — НЕ захватываем внутрь \"ПД ...\"\n",
        "    leaf_pattern = r'(л\\.\\s*(?:(?!ПД).)*?)(?=(?:ПД|;|$))'\n",
        "\n",
        "    token_pattern = re.compile(\n",
        "        r'(ПД\\s*\\d+\\s*[–—-]\\s*ПД\\s*\\d+)|'  # ПД A — ПД B\n",
        "        r'(ПД\\s*\\d+)|'                      # отдельный ПД\n",
        "        + leaf_pattern,                    # блок листов до следующего ПД/; или конца\n",
        "        flags=re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    tokens = []\n",
        "    pd_has_sheets = {}\n",
        "    current_pd = None\n",
        "\n",
        "    # ---------- PASS 1: токенизация ----------\n",
        "    for m in token_pattern.finditer(s):\n",
        "        interval = m.group(1)\n",
        "        pd_token = m.group(2)\n",
        "        block = m.group(3)\n",
        "\n",
        "        if interval:\n",
        "            nums = re.findall(r'ПД\\s*(\\d+)', interval)\n",
        "            if len(nums) == 2:\n",
        "                a, b = map(int, nums)\n",
        "                tokens.append({\"type\": \"interval\", \"a\": a, \"b\": b})\n",
        "                step = 1 if b >= a else -1\n",
        "                for n in range(a, b + step, step):\n",
        "                    pd_has_sheets[f\"ПД {n}\"] = True\n",
        "            current_pd = None\n",
        "            continue\n",
        "\n",
        "        if pd_token:\n",
        "            num = re.search(r'ПД\\s*(\\d+)', pd_token).group(1)\n",
        "            pd_name = f\"ПД {num}\"\n",
        "            current_pd = pd_name\n",
        "            if pd_name not in pd_has_sheets:\n",
        "                pd_has_sheets[pd_name] = False\n",
        "            tokens.append({\"type\": \"pd\", \"pd\": pd_name})\n",
        "            continue\n",
        "\n",
        "        if block and current_pd is not None:\n",
        "            preview = extract_sheets(block)\n",
        "            if preview:\n",
        "                tokens.append({\"type\": \"sheets\", \"pd\": current_pd, \"block\": block})\n",
        "                pd_has_sheets[current_pd] = True\n",
        "\n",
        "    # ---------- PASS 2: собираем результат по порядку ----------\n",
        "    result = []\n",
        "\n",
        "    for t in tokens:\n",
        "        kind = t[\"type\"]\n",
        "\n",
        "        if kind == \"interval\":\n",
        "            result.extend(expand_pd_interval(t[\"a\"], t[\"b\"]))\n",
        "\n",
        "        elif kind == \"pd\":\n",
        "            pd_name = t[\"pd\"]\n",
        "            if not pd_has_sheets.get(pd_name, False):\n",
        "                result.extend(generate_full_pd(pd_name))\n",
        "\n",
        "        elif kind == \"sheets\":\n",
        "            expanded = extract_sheets(t[\"block\"])\n",
        "            for sref in expanded:\n",
        "                result.append(f\"{t['pd']} л. {sref}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def convert_file(path_in, path_out):\n",
        "    df = pd.read_csv(path_in, encoding='utf-8')\n",
        "    df['autographs_parsed'] = df['autographs'].apply(\n",
        "        lambda x: '; '.join(parse_record(x))\n",
        "    )\n",
        "    df.to_csv(path_out, index=False, encoding='utf-8-sig')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_file(\"variant_5.csv\", \"parsed_variant_5.csv\")"
      ],
      "metadata": {
        "id": "NecVlao3aCao"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}