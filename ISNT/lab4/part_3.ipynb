{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98fb91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import requests\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Optional, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "027960e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Библиотеки импортированы, структуры данных определены.\n"
     ]
    }
   ],
   "source": [
    "Coordinate = Tuple[float, float]\n",
    "API_TIMEOUT = 60\n",
    "\n",
    "@dataclass\n",
    "class MapEntity:\n",
    "    title: str\n",
    "    category: str\n",
    "    points: List[Coordinate] = field(default_factory=list)\n",
    "\n",
    "print(\"Библиотеки импортированы, структуры данных определены.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6547195",
   "metadata": {},
   "source": [
    "Данный класс отвечает за построение газетира. Он:\n",
    "    •Загружает станции метро через фильтры  public_transport=station,  railway=station,  railway=subway_entrance \n",
    "\t•Загружает дороги через тег highway  с геометрией\n",
    "\t•Нормализует названия (приведение к нижнему регистру, разворачивание аббревиатур)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ea3a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gazetteer:\n",
    "    ENDPOINT = \"http://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.subway_stations: Dict[str, MapEntity] = {}\n",
    "        self.street_network: Dict[str, MapEntity] = {}\n",
    "        self._initialize_database()\n",
    "\n",
    "    def _initialize_database(self):\n",
    "        print(\"Начало загрузки геоданных (SPB)...\")\n",
    "        self._fetch_transport_nodes()\n",
    "        self._fetch_street_ways()\n",
    "        print(f\"База готова: Метро — {len(self.subway_stations)}, Улиц — {len(self.street_network)}\")\n",
    "\n",
    "    def _execute_query(self, osm_ql: str):\n",
    "        try:\n",
    "            response = requests.get(self.ENDPOINT, params={\"data\": osm_ql}, timeout=API_TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка соединения с OSM: {e}\")\n",
    "            return {\"elements\": []}\n",
    "\n",
    "    def _fetch_transport_nodes(self):\n",
    "        ql_query = \"\"\"\n",
    "        [out:json][timeout:60];\n",
    "        area[name=\"Санкт-Петербург\"]->.searchArea;\n",
    "        (\n",
    "          node[\"public_transport\"=\"station\"][\"station\"=\"subway\"](area.searchArea);\n",
    "          node[\"railway\"=\"station\"][\"station\"=\"subway\"](area.searchArea);\n",
    "          node[\"railway\"=\"subway_entrance\"](area.searchArea);\n",
    "        );\n",
    "        out center;\n",
    "        \"\"\"\n",
    "        raw_data = self._execute_query(ql_query)\n",
    "\n",
    "        for item in raw_data.get(\"elements\", []):\n",
    "            if item.get(\"type\") != \"node\": \n",
    "                continue\n",
    "            \n",
    "            lbl = item.get(\"tags\", {}).get(\"name\", \"\").strip()\n",
    "            if not lbl: \n",
    "                continue\n",
    "\n",
    "            clean_lbl = self.sanitize_name(lbl)\n",
    "            geo_pt = (item.get(\"lat\"), item.get(\"lon\"))\n",
    "\n",
    "            if None in geo_pt: \n",
    "                continue\n",
    "\n",
    "            if clean_lbl not in self.subway_stations:\n",
    "                self.subway_stations[clean_lbl] = MapEntity(\n",
    "                    title=lbl,\n",
    "                    category=\"metro\",\n",
    "                    points=[geo_pt]\n",
    "                )\n",
    "\n",
    "    def _fetch_street_ways(self):\n",
    "        ql_query = \"\"\"\n",
    "        [out:json][timeout:120];\n",
    "        area[name=\"Санкт-Петербург\"]->.searchArea;\n",
    "        (\n",
    "          way[\"highway\"][\"name\"](area.searchArea);\n",
    "        );\n",
    "        out geom;\n",
    "        \"\"\"\n",
    "        raw_data = self._execute_query(ql_query)\n",
    "\n",
    "        for item in raw_data.get(\"elements\", []):\n",
    "            if item.get(\"type\") != \"way\": \n",
    "                continue\n",
    "            \n",
    "            tags = item.get(\"tags\", {})\n",
    "            lbl = tags.get(\"name\", \"\").strip()\n",
    "            if not lbl: \n",
    "                continue\n",
    "\n",
    "            geom = item.get(\"geometry\", [])\n",
    "            if not geom: \n",
    "                continue\n",
    "\n",
    "            path = [(p[\"lat\"], p[\"lon\"]) for p in geom]\n",
    "            clean_lbl = self.sanitize_name(lbl)\n",
    "\n",
    "            if clean_lbl in self.street_network:\n",
    "                self.street_network[clean_lbl].points.extend(path)\n",
    "            else:\n",
    "                self.street_network[clean_lbl] = MapEntity(\n",
    "                    title=lbl,\n",
    "                    category=\"road\",\n",
    "                    points=path\n",
    "                )\n",
    "\n",
    "    def sanitize_name(self, raw_text: str):\n",
    "        text = raw_text.lower().strip()\n",
    "        replacements = [\n",
    "            (\"ул.\", \"улица\"), (\"пр.\", \"проспект\"), (\"пр-т\", \"проспект\"),\n",
    "            (\"наб.\", \"набережная\"), (\"пл.\", \"площадь\"), (\"б-р\", \"бульвар\"),\n",
    "            (\"ш.\", \"шоссе\"), (\"пер.\", \"переулок\")\n",
    "        ]\n",
    "        for old, new in replacements:\n",
    "            text = text.replace(old, new)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbd42e",
   "metadata": {},
   "source": [
    "Данный класс реализует алгоритм извлечения координат:\n",
    "\n",
    "Шаг 1: Извлечение топонимов\n",
    "\t•Метод: регулярные выражения для русского языка\n",
    "\t•Паттерны для улиц:  улица + название ,  проспект + название ,  набережная + название \n",
    "\t•Паттерны для метро:  метро «название» ,  станция метро название \n",
    "\t•Паттерны для пересечений:  на пересечении X с Y ,  у перекрестка X и Y \n",
    "Шаг 2: Сопоставление с газетиром\n",
    "\tНечёткое сопоставление через:\n",
    "\t\t•Словарь алиасов для распространённых вариантов написания\n",
    "\t\t•Точное совпадение нормализованных названий\n",
    "\t\t•Токенизация и подсчёт пересечений слов\n",
    "\t\t•Поиск длинных общих подстрок (>3 символа)\n",
    "Шаг 3: Определение пространственных отношений\n",
    "\t•Поиск ключевых слов: “пересечение”, “перекресток”, “съезд”\n",
    "\t•Извлечение пар улиц из контекста\n",
    "\t•Расчёт точки пересечения: перебор точек двух полилиний, поиск минимального расстояния по формуле Haversine\n",
    "Шаг 4: Агрегация координат\n",
    "\t•Если найдено пересечение → возвращаем его координаты\n",
    "\t•Иначе — вычисляем взвешенный центроид:\n",
    "\t•Станции метро весят ×10 (подбирал в ручную)\n",
    "\t•Улицы усредняются по всем точкам\n",
    "\t•Финальная координата = центр масс всех найденных объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8e97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncidentGeoParser:\n",
    "    def __init__(self, verbose_mode: bool = False):\n",
    "        self.map_provider = Gazetteer()\n",
    "        self.verbose = verbose_mode\n",
    "        self._compile_patterns()\n",
    "\n",
    "    def _compile_patterns(self):\n",
    "        self.markers_intersection = {\n",
    "            \"пересечение\", \"перекресток\", \"пересечения\", \"перекрестке\",\n",
    "            \"на пересечении\", \"у пересечения\", \"на перекрестке\", \"у перекрестка\"\n",
    "        }\n",
    "\n",
    "        self.regex_streets = [\n",
    "            r\"(улиц[аеиыё]?\\s+[а-яё][а-яё\\s\\-\\.№]+?)(?:\\s|,|\\.|\\d)\",\n",
    "            r\"(проспект[еаиы]?\\s+[а-яё][а-яё\\s\\-\\.№]+?)(?:\\s|,|\\.|\\d)\",\n",
    "            r\"(шоссе\\s+[а-яё][а-яё\\s\\-\\.№]+?)(?:\\s|,|\\.|\\d)\",\n",
    "            r\"(набережн[аояыеи]+\\s+[а-яё][а-яё\\s\\-\\.№]+?)(?:\\s|,|\\.|\\d)\",\n",
    "            r\"(площад[иьья]+\\s+[а-яё][а-яё\\s\\-\\.№]+?)(?:\\s|,|\\.|\\d)\",\n",
    "            r\"(бульвар[еаиы]?\\s+[а-яё][а-яё\\s\\-\\.№]+?)(?:\\s|,|\\.|\\d)\",\n",
    "            r\"(переулок[еаиы]?\\s+[а-яё][а-яё\\s\\-\\.№]+?)(?:\\s|,|\\.|\\d)\",\n",
    "            r\"([а-яё]+ской)\\s+улиц[еи]\",\n",
    "        ]\n",
    "\n",
    "        self.regex_metro = [\n",
    "            r\"(станци[ияйе]\\s+метро\\s+[«\\\"„][а-яё\\s\\-\\.]+[»\\\"”])\",\n",
    "            r\"(метро\\s+[«\\\"„][а-яё\\s\\-\\.]+[»\\\"”])\",\n",
    "            r\"у\\s+метро\\s+[«\\\"„]?([а-яё\\s\\-\\.]+)[»\\\"”]?\",\n",
    "            r\"станци[ияйе]\\s+[«\\\"„]?([а-яё\\s\\-\\.]+)[»\\\"”]?\",\n",
    "            r\"([а-яё\\s\\-\\.]+)\\s+метро(?!\\s+[а-яё])\",\n",
    "        ]\n",
    "        \n",
    "        self.regex_crossings = [\n",
    "            r\"(на\\s+пересечени[ииеё]\\s+[а-яё\\s\\-\\.]+\\s+с\\s+[а-яё\\s\\-\\.]+)\",\n",
    "            r\"(у\\s+пересечени[ияя]\\s+[а-яё\\s\\-\\.]+\\s+с\\s+[а-яё\\s\\-\\.]+)\",\n",
    "            r\"(на\\s+перекрестк[еёа]\\s+[а-яё\\s\\-\\.]+\\s+с\\s+[а-яё\\s\\-\\.]+)\",\n",
    "            r\"(у\\s+перекрестк[аеё]\\s+[а-яё\\s\\-\\.]+\\s+с\\s+[а-яё\\s\\-\\.]+)\",\n",
    "            r\"(пересечени[ея]\\s+[а-яё\\s\\-\\.]+\\s+и\\s+[а-яё\\s\\-\\.]+)\",\n",
    "            r\"(перекресток\\s+[а-яё\\s\\-\\.]+\\s+и\\s+[а-яё\\s\\-\\.]+)\",\n",
    "        ]\n",
    "\n",
    "    def solve_coordinates(self, content: str):\n",
    "        if self.verbose:\n",
    "            print(f\"Анализ текста: {content[:300]}...\")\n",
    "\n",
    "        found_entities = self._scan_text(content)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Обнаружено объектов: {len(found_entities)}\")\n",
    "            for ent in found_entities:\n",
    "                print(f\" -> {ent.title} [{ent.category}]\")\n",
    "\n",
    "        if not found_entities:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        cross_points = self._analyze_intersections(content, found_entities)\n",
    "\n",
    "        if cross_points:\n",
    "            res = self._calculate_intersection_center(cross_points[0])\n",
    "            if self.verbose:\n",
    "                print(f\"Координаты (пересечение): {res}\")\n",
    "            return res\n",
    "\n",
    "        res = self._calculate_centroid(found_entities)\n",
    "        if self.verbose:\n",
    "            print(f\"Координаты (центроид): {res}\")\n",
    "        return res\n",
    "\n",
    "    def _scan_text(self, text: str) -> List[MapEntity]:\n",
    "        buffer_lower = text.lower()\n",
    "        unique_results: List[MapEntity] = []\n",
    "\n",
    "        for r_pat in self.regex_streets:\n",
    "            for m in re.finditer(r_pat, buffer_lower, re.IGNORECASE):\n",
    "                raw_fragment = m.group(1).strip()\n",
    "                match = self._fuzzy_lookup(raw_fragment, \"road\")\n",
    "                if match and match not in unique_results:\n",
    "                    unique_results.append(match)\n",
    "\n",
    "        for r_pat in self.regex_metro:\n",
    "            for m in re.finditer(r_pat, buffer_lower, re.IGNORECASE):\n",
    "                raw_fragment = m.group(1).strip()\n",
    "                cleaned = re.sub(r\"(станци[ияйе]\\s+)?(метро\\s+)?[«\\\"„»”]?\", \"\", raw_fragment).strip()\n",
    "                if not cleaned: \n",
    "                    continue\n",
    "                match = self._fuzzy_lookup(cleaned, \"metro\")\n",
    "                if match and match not in unique_results:\n",
    "                    unique_results.append(match)\n",
    "\n",
    "        return unique_results\n",
    "\n",
    "    def _fuzzy_lookup(self, query_str: str, mode: str):\n",
    "        normalized_q = self.map_provider.sanitize_name(query_str)\n",
    "        \n",
    "        aliases = {\n",
    "            \"кантемировской\": \"кантемировская улица\", \"красина\": \"улица красина\",\n",
    "            \"андреевской\": \"андреевская улица\", \"казакова\": \"маршала казакова\",\n",
    "            \"героев\": \"проспект героев\", \"авиаконструкторов\": \"проспект авиаконструкторов\",\n",
    "            \"шуваловского\": \"шуваловский проспект\", \"приморского\": \"приморское шоссе\",\n",
    "            \"мосина\": \"улица мосина\", \"ополчения\": \"проспект народного ополчения\",\n",
    "            \"голикова\": \"улица ленина голикова\", \"конюшенной\": \"большая конюшенная улица\",\n",
    "            \"большевиков\": \"проспект большевиков\", \"дыбенко\": \"улица дыбенко\",\n",
    "            \"коллонтай\": \"улица коллонтай\",\n",
    "        }\n",
    "\n",
    "        for k, v in aliases.items():\n",
    "            if k in normalized_q:\n",
    "                mapped_norm = self.map_provider.sanitize_name(v)\n",
    "                target_dict = self.map_provider.subway_stations if mode == \"metro\" else self.map_provider.street_network\n",
    "                if mapped_norm in target_dict:\n",
    "                    return target_dict[mapped_norm]\n",
    "\n",
    "        dataset = self.map_provider.subway_stations if mode == \"metro\" else self.map_provider.street_network\n",
    "\n",
    "        if normalized_q in dataset:\n",
    "            return dataset[normalized_q]\n",
    "\n",
    "        best_candidate: Optional[MapEntity] = None\n",
    "        max_overlap = 0\n",
    "        q_tokens = set(normalized_q.split())\n",
    "\n",
    "        for db_name, entity in dataset.items():\n",
    "            db_tokens = set(db_name.split())\n",
    "            overlap_cnt = len(q_tokens & db_tokens)\n",
    "            \n",
    "            has_long_substr = any(token for token in q_tokens if len(token) > 3 and token in db_name)\n",
    "\n",
    "            if overlap_cnt > max_overlap and overlap_cnt >= 1:\n",
    "                max_overlap = overlap_cnt\n",
    "                best_candidate = entity\n",
    "            elif has_long_substr and max_overlap == 0 and best_candidate is None:\n",
    "                best_candidate = entity\n",
    "\n",
    "        return best_candidate\n",
    "\n",
    "    def _analyze_intersections(self, text: str, entities: List[MapEntity]):\n",
    "        buffer_lower = text.lower()\n",
    "        found_pairs = []\n",
    "        \n",
    "        roads_only = [e for e in entities if e.category == \"road\"]\n",
    "\n",
    "        for pat in self.regex_crossings:\n",
    "            for m in re.finditer(pat, buffer_lower, re.IGNORECASE):\n",
    "                phrase = m.group(1)\n",
    "                r1, r2 = self._extract_roads_from_phrase(phrase, roads_only)\n",
    "                if r1 and r2:\n",
    "                    found_pairs.append((r1, r2))\n",
    "                    break\n",
    "\n",
    "        if not found_pairs and len(roads_only) >= 2:\n",
    "            if any(k in buffer_lower for k in self.markers_intersection):\n",
    "                found_pairs.append((roads_only[0], roads_only[1]))\n",
    "\n",
    "        return found_pairs\n",
    "\n",
    "    def _extract_roads_from_phrase(self, phrase: str, road_list: List[MapEntity]):\n",
    "        candidates = []\n",
    "        phrase_tokens = set(phrase.lower().split())\n",
    "\n",
    "        for r in road_list:\n",
    "            r_tokens = set(r.title.lower().split())\n",
    "            intersection_size = len(phrase_tokens & r_tokens)\n",
    "            if intersection_size > 0:\n",
    "                candidates.append((intersection_size, r))\n",
    "\n",
    "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        if len(candidates) >= 2:\n",
    "            return candidates[0][1], candidates[1][1]\n",
    "        \n",
    "        if len(road_list) >= 2:\n",
    "            return road_list[0], road_list[1]\n",
    "            \n",
    "        return None, None\n",
    "\n",
    "    def _calculate_intersection_center(self, pair: Tuple[MapEntity, MapEntity]) -> Coordinate:\n",
    "        obj_a, obj_b = pair\n",
    "        closest_pt = (0.0, 0.0)\n",
    "        min_distance = float(\"inf\")\n",
    "\n",
    "        points_a_subset = obj_a.points[::2] if len(obj_a.points) > 10 else obj_a.points\n",
    "        points_b_subset = obj_b.points[::2] if len(obj_b.points) > 10 else obj_b.points\n",
    "\n",
    "        for p_a in points_a_subset:\n",
    "            for p_b in points_b_subset:\n",
    "                dist = self.haversine_distance(p_a, p_b)\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    mid_lat = (p_a[0] + p_b[0]) / 2\n",
    "                    mid_lon = (p_a[1] + p_b[1]) / 2\n",
    "                    closest_pt = (mid_lat, mid_lon)\n",
    "\n",
    "        return closest_pt\n",
    "\n",
    "    def _calculate_centroid(self, entities: List[MapEntity]) -> Coordinate:\n",
    "        if not entities:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        collection_pts: List[Coordinate] = []\n",
    "        \n",
    "        for e in entities:\n",
    "            if e.category == \"metro\":\n",
    "                collection_pts.extend(e.points * 10)\n",
    "            elif e.points:\n",
    "                avg_lat = sum(p[0] for p in e.points) / len(e.points)\n",
    "                avg_lon = sum(p[1] for p in e.points) / len(e.points)\n",
    "                collection_pts.append((avg_lat, avg_lon))\n",
    "\n",
    "        if not collection_pts:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        final_lat = sum(p[0] for p in collection_pts) / len(collection_pts)\n",
    "        final_lon = sum(p[1] for p in collection_pts) / len(collection_pts)\n",
    "        return final_lat, final_lon\n",
    "\n",
    "    @staticmethod\n",
    "    def haversine_distance(pt1: Coordinate, pt2: Coordinate):\n",
    "        R_EARTH = 6371000\n",
    "        lat1, lon1 = map(math.radians, pt1)\n",
    "        lat2, lon2 = map(math.radians, pt2)\n",
    "\n",
    "        delta_phi = lat2 - lat1\n",
    "        delta_lambda = lon2 - lon1\n",
    "\n",
    "        a = (math.sin(delta_phi / 2) ** 2 +\n",
    "             math.cos(lat1) * math.cos(lat2) * math.sin(delta_lambda / 2) ** 2)\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        return R_EARTH * c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3a426",
   "metadata": {},
   "source": [
    "Реализованы три метрики сравнения предсказанных и реальных координат:\n",
    "\t1.\tHaversine error — реальное расстояние по геодезической линии\n",
    "\t2.\tRMSE — среднеквадратическая ошибка в метрах\n",
    "\t3.\tManhattan distance — сумма отклонений по широте и долготе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d808d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detailed_metrics(real: Coordinate, predicted: Coordinate):\n",
    "    dist = IncidentGeoParser.haversine_distance(real, predicted)\n",
    "\n",
    "    d_lat_m = (real[0] - predicted[0]) * 111000\n",
    "    d_lon_m = (real[1] - predicted[1]) * 111000 * math.cos(math.radians(real[0]))\n",
    "\n",
    "    return {\n",
    "        \"haversine_error\": dist,\n",
    "        \"rmse_error\": math.sqrt(d_lat_m**2 + d_lon_m**2),\n",
    "        \"manhattan_error\": abs(d_lat_m) + abs(d_lon_m)\n",
    "    }\n",
    "\n",
    "def run_validation_loop(dataset: List[Dict], engine: IncidentGeoParser):\n",
    "    total_dev = 0.0\n",
    "    valid_counts = 0\n",
    "    error_log: List[float] = []\n",
    "\n",
    "    print(f\"Запуск валидации на {len(dataset)} примерах...\\n\")\n",
    "\n",
    "    for i, entry in enumerate(dataset, 1):\n",
    "        txt = entry[\"text\"]\n",
    "        truth = tuple(entry[\"rta_coords\"])\n",
    "\n",
    "        print(f\"--- Case #{i} ---\")\n",
    "        print(f\"Text snippet: {txt[:100]}...\")\n",
    "        print(f\"GT: {truth}\")\n",
    "\n",
    "        guess = engine.solve_coordinates(txt)\n",
    "        print(f\"Prediction: {guess}\")\n",
    "\n",
    "        if guess != (0.0, 0.0):\n",
    "            deviation = engine.haversine_distance(truth, guess)\n",
    "            print(f\"Deviation: {deviation:.2f} m\")\n",
    "            \n",
    "            total_dev += deviation\n",
    "            valid_counts += 1\n",
    "            error_log.append(deviation)\n",
    "        else:\n",
    "            print(\"[WARN] Координаты не извлечены.\")\n",
    "            error_log.append(float(\"inf\"))\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"=== Результаты тестирования ===\")\n",
    "    print(f\"Успешно обработано: {valid_counts} / {len(dataset)}\")\n",
    "\n",
    "    if valid_counts > 0:\n",
    "        finite_errs = [x for x in error_log if x != float(\"inf\")]\n",
    "        avg_val = total_dev / valid_counts\n",
    "        min_val = min(finite_errs) if finite_errs else float(\"nan\")\n",
    "        max_val = max(finite_errs) if finite_errs else float(\"nan\")\n",
    "        \n",
    "        print(f\"Mean Error: {avg_val:.2f} m\")\n",
    "        print(f\"Min Error: {min_val:.2f} m\")\n",
    "        print(f\"Max Error: {max_val:.2f} m\")\n",
    "        \n",
    "        accurate = sum(1 for x in finite_errs if x < 500)\n",
    "        print(f\"High precision (<500m): {accurate} cases\")\n",
    "    else:\n",
    "        print(\"Нет успешных предсказаний.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b05d0f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало загрузки геоданных (SPB)...\n",
      "База готова: Метро — 91, Улиц — 4017\n",
      "Запуск валидации на 10 примерах...\n",
      "\n",
      "--- Case #1 ---\n",
      "Text snippet: Авария с участием спорткара, грузовика и велосипедиста произошла утром 29 августа в Выборгском район...\n",
      "GT: (59.984386, 30.335297)\n",
      "Prediction: (59.98432879935487, 30.340133430322577)\n",
      "Deviation: 269.10 m\n",
      "\n",
      "--- Case #2 ---\n",
      "Text snippet: В Красногвардейском районе Петербурга водитель не справился с управлением машиной на закруглении дор...\n",
      "GT: (59.97056, 30.496609)\n",
      "Prediction: (59.95078308194901, 30.477721136964433)\n",
      "Deviation: 2437.49 m\n",
      "\n",
      "--- Case #3 ---\n",
      "Text snippet: Петербургская полиция разбирается в обстоятельствах ДТП в Красносельском районе города. В ночь на 16...\n",
      "GT: (59.868634, 30.168545)\n",
      "Prediction: (60.017582454717, 30.270708954716987)\n",
      "Deviation: 17512.40 m\n",
      "\n",
      "--- Case #4 ---\n",
      "Text snippet: Водителя на «Киа Рио» сегодня около трех часов ночи пытались остановить сотрудники отдельного специа...\n",
      "GT: (60.025162, 30.22853)\n",
      "Prediction: (60.0259612747788, 30.222856046460183)\n",
      "Deviation: 327.50 m\n",
      "\n",
      "--- Case #5 ---\n",
      "Text snippet: Два легковых автомобиля жестко столкнулись поздно вечером 19 июля в Курортном районе Петербурга. Чет...\n",
      "GT: (60.088041, 29.965847)\n",
      "Prediction: (60.050960700000005, 30.13888805)\n",
      "Deviation: 10448.37 m\n",
      "\n",
      "--- Case #6 ---\n",
      "Text snippet: Полиция проводит проверку по факту наезда на десятилетнюю школьницу в Кировском районе Петербурга. А...\n",
      "GT: (59.831841, 30.252345)\n",
      "Prediction: (59.83565233191134, 30.230149716040973)\n",
      "Deviation: 1310.61 m\n",
      "\n",
      "--- Case #7 ---\n",
      "Text snippet: Авария с участием автомобиля и мотоцикла произошла вечером 11 июля в центре Петербурга. Около 22:00 ...\n",
      "GT: (59.935974, 30.322013)\n",
      "Prediction: (59.89838410322579, 30.4148741139785)\n",
      "Deviation: 6652.77 m\n",
      "\n",
      "--- Case #8 ---\n",
      "Text snippet: Легковушка на полном ходу врезалась в зеленый автобус у станции метро «Улица Дыбенко», ДТП попало на...\n",
      "GT: (59.906773, 30.482733)\n",
      "Prediction: (59.90764279033794, 30.48203809288434)\n",
      "Deviation: 104.19 m\n",
      "\n",
      "--- Case #9 ---\n",
      "Text snippet: В Невском районе ДТП произошло из-за психанувшего водителя легкового автомобиля. Мужчина вступил в с...\n",
      "GT: (59.919226, 30.46734)\n",
      "Prediction: (59.918627712719, 30.466076504368402)\n",
      "Deviation: 96.87 m\n",
      "\n",
      "--- Case #10 ---\n",
      "Text snippet: Лобовое столкновение машин произошло у метро «Беговая», на месте ДТП – реанимобиль. Кроссовер выехал...\n",
      "GT: (59.987916, 30.202486)\n",
      "Prediction: (59.987266099999985, 30.202193599999994)\n",
      "Deviation: 74.07 m\n",
      "\n",
      "=== Результаты тестирования ===\n",
      "Успешно обработано: 10 / 10\n",
      "Mean Error: 3923.34 m\n",
      "Min Error: 74.07 m\n",
      "Max Error: 17512.40 m\n",
      "High precision (<500m): 5 cases\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        with open(\"rta_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_json = json.load(f)\n",
    "            validation_set = raw_json[\"text_list\"]\n",
    "            \n",
    "        processor = IncidentGeoParser(verbose_mode=False)\n",
    "\n",
    "        run_validation_loop(validation_set, processor)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Файл 'rta_texts.json' не найден. Проверьте путь.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658ba72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
